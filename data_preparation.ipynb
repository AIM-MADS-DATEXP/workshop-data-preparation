{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data preparation and cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import pandas as pd\n",
    "from pandas.api.types import is_numeric_dtype\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Detecting problems"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PassengerId</th>\n",
       "      <th>Survived</th>\n",
       "      <th>Pclass</th>\n",
       "      <th>Age</th>\n",
       "      <th>SibSp</th>\n",
       "      <th>Parch</th>\n",
       "      <th>Fare</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>1063.000000</td>\n",
       "      <td>1063.000000</td>\n",
       "      <td>1063.000000</td>\n",
       "      <td>855.000000</td>\n",
       "      <td>1063.000000</td>\n",
       "      <td>1063.000000</td>\n",
       "      <td>1063.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>447.238946</td>\n",
       "      <td>0.379116</td>\n",
       "      <td>2.301035</td>\n",
       "      <td>31.216480</td>\n",
       "      <td>0.512700</td>\n",
       "      <td>0.382879</td>\n",
       "      <td>32.547749</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>260.770475</td>\n",
       "      <td>0.485395</td>\n",
       "      <td>0.837936</td>\n",
       "      <td>18.641281</td>\n",
       "      <td>1.083856</td>\n",
       "      <td>0.807899</td>\n",
       "      <td>48.902782</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.420000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>224.500000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>21.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>7.910400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>444.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>28.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>14.458300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>673.500000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>39.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>31.275000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>891.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>130.000000</td>\n",
       "      <td>8.000000</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>512.329200</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       PassengerId     Survived       Pclass         Age        SibSp  \\\n",
       "count  1063.000000  1063.000000  1063.000000  855.000000  1063.000000   \n",
       "mean    447.238946     0.379116     2.301035   31.216480     0.512700   \n",
       "std     260.770475     0.485395     0.837936   18.641281     1.083856   \n",
       "min       1.000000     0.000000     1.000000    0.420000     0.000000   \n",
       "25%     224.500000     0.000000     2.000000   21.000000     0.000000   \n",
       "50%     444.000000     0.000000     3.000000   28.000000     0.000000   \n",
       "75%     673.500000     1.000000     3.000000   39.000000     1.000000   \n",
       "max     891.000000     1.000000     3.000000  130.000000     8.000000   \n",
       "\n",
       "             Parch         Fare  \n",
       "count  1063.000000  1063.000000  \n",
       "mean      0.382879    32.547749  \n",
       "std       0.807899    48.902782  \n",
       "min       0.000000     0.000000  \n",
       "25%       0.000000     7.910400  \n",
       "50%       0.000000    14.458300  \n",
       "75%       0.000000    31.275000  \n",
       "max       6.000000   512.329200  "
      ]
     },
     "execution_count": 141,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dft = pd.read_csv(\"data/titanic_processed.csv\")\n",
    "dft.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Empty values\n",
    "\n",
    "The first and most obvious thing to check is whether there are empty values in the data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Column PassengerId  has    0 NaN values\n",
      "Column Survived     has    0 NaN values\n",
      "Column Pclass       has    0 NaN values\n",
      "Column Name         has    0 NaN values\n",
      "Column Sex          has    0 NaN values\n",
      "Column Age          has  208 NaN values\n",
      "Column SibSp        has    0 NaN values\n",
      "Column Parch        has    0 NaN values\n",
      "Column Ticket       has    0 NaN values\n",
      "Column Fare         has    0 NaN values\n",
      "Column Cabin        has  816 NaN values\n",
      "Column Embarked     has    3 NaN values\n"
     ]
    }
   ],
   "source": [
    "for col in dft.columns:\n",
    "    print(f'Column {col: <12} has {dft[col].isna().sum(): 4d} NaN values')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Duplicates\n",
    "\n",
    "Records have an unpleasant tendency to be duplicated. Luckily Pandas can help detect this.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of duplicates is 172\n"
     ]
    }
   ],
   "source": [
    "print(f'Number of duplicates is {dft.duplicated( keep='first').sum()}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Nonsensical values\n",
    "\n",
    "Let's check each column to see if the values make sense.\n",
    "\n",
    "For numerical columns we do this by checking the range of values while for text columns we list the unique values.\n",
    "\n",
    "This data set has one column for which this is impossible: we likely have no way of determining whether a value in the \"Name\" column is plausible or not. The same is true for the \"Ticket\" column which holds the ticket number and for the \"Cabin\" column which holds the cabin number. It's probably also not useful to check what's in the \"PassengerId\" column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Column \"Survived\" has range 0 - 1\n",
      "Column \"Pclass\" has range 1 - 3\n",
      "Column Sex has values ['male' 'female' 'caprio' 'fmale'] \n",
      "Column \"Age\" has range 0.42 - 130.0\n",
      "Column \"SibSp\" has range 0 - 8\n",
      "Column \"Parch\" has range 0 - 6\n",
      "Column \"Fare\" has range 0.0 - 512.3292\n",
      "Column Embarked has values ['S' 'C' 'Q' nan] \n"
     ]
    }
   ],
   "source": [
    "columns_to_skip = ['PassengerId', 'Name', 'Ticket', 'Cabin']\n",
    "for col in dft.columns:\n",
    "    if col in columns_to_skip:\n",
    "        continue\n",
    "    if is_numeric_dtype(dft[col]):\n",
    "        print(f'Column \"{col}\" has range {dft[col].min()} - {dft[col].max()}')\n",
    "    else:\n",
    "        print(f'Column {col} has values {dft[col].unique()} ')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fixing problems"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Duplicates\n",
    "\n",
    "First of all we want to get rid of the duplicate records Pandas detected for us above.\n",
    "\n",
    "How this works:\n",
    "- `df.duplicated(keep = 'first')` returns a list of True / False values. \"True\" if the record is considered to be a duplicate, \"False\" if it's not.\n",
    "- We then use this list to filter the original dataframe. Normally, applying a list of True / False values as a filter (`df[list_of_true_false]`) will return only those records for which the list has a value of \"True\". In this case, \"True\" means the record is a duplicate. What we actually want is to retain only those records for which the list has a value of False. We accomplish this by using the `~` operator (logical NOT) before the filter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After de-duplicating we are left with 0 duplicates\n"
     ]
    }
   ],
   "source": [
    "dft_clean = dft[~dft.duplicated( keep='first')]\n",
    "# Verify there are no more duplicates:\n",
    "print(f'After de-duplicating we are left with {dft_clean.duplicated(keep = 'first').sum()} duplicates')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dealing with nonsensical values\n",
    "\n",
    "We have found two columns with nonsensical values:\n",
    "- Not all values in the \"Sex\" column are correct. Some of these incorrect values are obvious typos, others are plain hot garbage.\n",
    "- The \"Age\" column has values ranging from 0.4 to 130. Even today people rarely age to more than 100 years, let alone in 1912."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Dealing with nonsensical values in the \"Sex\" column\n",
    "\n",
    "The obvious typos in the \"Sex\" column are easy enough to fix.\n",
    "\n",
    "Again we use a filter (list of True / False) for this. The filter gives \"True\" for all records where the \"Sex\" column matches the condition, false where it doesn't. We can use this to update only those values that match the condition."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['male', 'female', 'caprio'], dtype=object)"
      ]
     },
     "execution_count": 146,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dft_clean.loc[dft_clean['Sex'] == 'fmale', 'Sex'] = 'female'\n",
    "# Verify that the typos are gone.\n",
    "dft_clean['Sex'].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some incorrect values are a bit more difficult to deal with. We really have no way of knowing what \"caprio\" means.\n",
    "\n",
    "To come up with a solution, we need to know how many records contain this value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number of records with a nonsensical value in the \"Sex\" column is 21 (out of a total of 891 records so 2.4% of the total\n"
     ]
    }
   ],
   "source": [
    "print(f'The number of records with a nonsensical value in the \"Sex\" column is {len(dft_clean[dft_clean['Sex'] == 'caprio'])} (out of a total of {len(dft_clean)} records so {(len(dft_clean[dft_clean['Sex'] == 'caprio']) / len(dft_clean)) * 100 :.1f}% of the total')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we know what we're dealing with, we have to make a decision. Throw them out or replace them with values that make sense? And if the latter: what does \"values that make sense\" mean? For numerical values we can use the mean, median or mode but for a categorical value we have to do more work.\n",
    "\n",
    "One option is simply to assign the value that is the most common. For this we need to know the counts for each value. The `value_counts` command tells us this (`group_by` with `count` would also work)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sex\n",
       "male      549\n",
       "female    321\n",
       "caprio     21\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 148,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dft_clean['Sex'].value_counts()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this case simply assigning the majority correct value is not the best option as the difference between the most common correct value and the least common correct value is not very large.\n",
    "\n",
    "An especially fancy way to handle this problem is to use something known as \"K-Nearest Neighbor Imputation\". For this technique, \"similarity\" is calculated between rows (by treating each row as a coördinate in a multi-dimensional space). We can then replace the incorrect values with the correct values in the nearest neighbors (hence the name of the algorithm - $K$ is simply the *number* of neighbors to consider).\n",
    "\n",
    "Of course we will have a problem if it turns out that the incorrect records only have neighbors that are incorrect themselves."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After imputation we have: [[  0.       1.       2.     ...   1.      79.65     1.    ]\n",
      " [  1.       3.       1.     ...   1.      12.475    1.    ]\n",
      " [  0.       3.       2.     ...   0.       8.05     1.    ]\n",
      " ...\n",
      " [  1.       1.       1.     ...   0.     135.6333   2.    ]\n",
      " [  1.       1.       2.     ...   1.     512.3292   2.    ]\n",
      " [  0.       3.       1.     ...   4.      21.075    1.    ]]\n",
      "After imputation we end up with the following unique values in the 'Sex' column: [2.  1.  1.5]\n",
      "Number of records with a value of 1.5: 7\n",
      "Sex\n",
      "2.0    558\n",
      "1.0    326\n",
      "1.5      7\n",
      "Name: count, dtype: int64\n",
      "After increasing the number of neighbors we end up with the following: \n",
      "Sex\n",
      "2.0    550\n",
      "1.0    321\n",
      "1.7      6\n",
      "1.4      3\n",
      "1.9      3\n",
      "1.5      3\n",
      "1.8      2\n",
      "1.6      2\n",
      "1.3      1\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.impute import KNNImputer\n",
    "# We need to create a new data frame containing only the numeric frames because KNNImputer only works with numeric values.\n",
    "# Reuse the columns_to_skip array we made above. Notice the nested list comprehension.\n",
    "dft_imp = dft_clean[[c for c in dft_clean.columns if c not in columns_to_skip]].copy()\n",
    "\n",
    "# We need to give the \"Sex\" column numerical values so we can use it with KNNImputer.\n",
    "# Notice we use a dict to translate string values to numbers. This allows us to reuse it later.\n",
    "translation = {\n",
    "    'female' : 1,\n",
    "    'male' : 2,\n",
    "    # The sex we want to impute needs to be set to NaN so KNNImputer can fill it.\n",
    "    'caprio' : np.nan,\n",
    "}\n",
    "for (orig, new) in translation.items():\n",
    "    dft_imp.loc[dft_imp['Sex'] == orig, 'Sex'] = new\n",
    "\n",
    "\n",
    "\n",
    "# The \"Embarked\" column also has non-numeric values. Since the exact value doesn't\n",
    "# matter, just give it any value\n",
    "emb_val = 1\n",
    "for val in dft_imp['Embarked'].unique():\n",
    "    dft_imp.loc[dft_imp['Embarked'] == val, 'Embarked'] = emb_val\n",
    "    emb_val +=1\n",
    "\n",
    "\n",
    "# Now run the imputer. Let's go with a low value for the number of neighbors to consider.\n",
    "imputer = KNNImputer(n_neighbors=2)\n",
    "imputed = imputer.fit_transform(dft_imp)\n",
    "print(f\"After imputation we have: {imputed}\")\n",
    "\n",
    "# Woops - we no longer have a data frame. We need to restore it.\n",
    "imputed = pd.DataFrame(imputed, columns = dft_imp.columns)\n",
    "\n",
    "# Let's see if it worked.\n",
    "print(f\"After imputation we end up with the following unique values in the 'Sex' column: {imputed['Sex'].unique()}\")\n",
    "\n",
    "# Ok. That didn't work. In some cases the imputer couldn't decide on a value so it took the average.\n",
    "# Let's see how often that happened:\n",
    "print(f\"Number of records with a value of 1.5: {len(imputed[imputed['Sex'] == 1.5])}\")\n",
    "print(imputed['Sex'].value_counts())\n",
    "\n",
    "# Let's see if increasing the number of neighbors helps:\n",
    "imputer = KNNImputer(n_neighbors=10)\n",
    "imputed = imputer.fit_transform(dft_imp)\n",
    "imputed = pd.DataFrame(imputed, columns = dft_imp.columns)\n",
    "print(f\"After increasing the number of neighbors we end up with the following: \\n{imputed['Sex'].value_counts()}\")\n",
    "\n",
    "# You can try other values for n_neighbors but you will find things never really improve.\n",
    "# Let's decide to solve the problem by rounding to the nearest integer.\n",
    "imputed['Sex'] = imputed['Sex'].round(0)\n",
    "\n",
    "# Finally translate the imputed sex column back to strings using the translation map above.\n",
    "# We need to change the data type to a string representation of an int to avoid complaints.\n",
    "imputed['Sex'] = imputed['Sex'].astype(int).astype(str)\n",
    "for stringval, number in translation.items():\n",
    "    number_as_string = str(number)\n",
    "    imputed.loc[imputed['Sex'] == number_as_string, 'Sex'] = stringval\n",
    "\n",
    "# And, of course, update the original Sex column to the new Sex column.\n",
    "# To make things easier to work with, create a new dataframe.\n",
    "# Notice use of reset_index. This is necessary to make sure the original columns\n",
    "# and the imputed column line up correctly.\n",
    "dft_clean_sex = dft_clean[[col for col in dft_clean.columns if col != 'Sex']].reset_index()\n",
    "dft_clean_sex['Sex'] = imputed.reset_index()['Sex']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Missing at Random\n",
    "Missing completely at Random\n",
    "Missing not at Random"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
